# EP-10553: Support Leader Election

* Issue: [#10553](https://github.com/kgateway-dev/kgateway/issues/10553)

## Background

Typically in leader election, a set of candidates for becoming leader is identified. These candidates all race to declare themselves the leader. One of the candidates wins and becomes the leader. Once the election is won, the leader continually "heartbeats" to renew their position as the leader, and the other candidates periodically make new attempts to become the leader. This ensures that a new leader is identified quickly, if the current leader fails for some reason.

Leader election is used to ensure only one instance of a component is running at any given time. This is used by control plane components like kube-controller-manager and kube-scheduler in HA configurations, where only one instance of the component should be actively running while the other instances are on stand-by.

The controller-runtime exposes the [leaderelection](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/leaderelection) package that is used to ensure that multiple copies of a controller manager can be run with only one active set of controllers, for active-passive HA. This can be used to support leader election in kgateway.

## Motivation

The current implementation of kgateway performs the following high level tasks :
- Deploy the gateway related resources (Gateway deployment, Service, GatewayClass, etc.)
- Write the status of the translated CRs (Gateway-API and kgateway CRs)
- Translate APIs to xDS snapshots to configure the gateway

To ensure high availability, kgateway needs to run in a multi-replica deployment to ensure continuous operation.
In such a scenario, let us examine at what happens if every pod performs all the aforementioned actions :
- Deploy the gateway and other resources : This will lead to gateway resources being re-deployment multiple times (as many as there are kgateway pods).
- Write the status of the translated CRs : This will lead to multiple pods writing the status of the same CR.
- Translate APIs to xDS snapshots : Each pod will end up with the same xDS snapshot.

From this we can see that only translation should be performed by all the pods, but the deployment and status updation should be done only by one pod. This results in no overlap between pods when it comes to contentious tasks.
The one pod to rule them all, ie: the leader, can be determined via [leaderelection](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/leaderelection) exposed when [configuring](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#Options) the [manager](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#Manager) provided by the controller-runtime module.
If the leader fails (crashes, unable to connect to the apiserver, etc.) it will not be able to renew the lease and another pod will take over leadership and assume the leader's tasks ensuring continuous operation.

### Goals
- Support leader election in kgateway to provide efficient HA.
- Ensure that contentious tasks are performed by the leader.
- All pods should perform translation ensuring consistent xDS provided to Envoy.
- Provide a simple configuration to disable leader election.
- Enable leader election by default.

### Non-Goals
- Define a new resource through which the leader will be determined.
- Define additional leader election options apart from a simple enable flag.
- Define an SLA / SLO to configure toleration for apiserver unavailability.
- Refactor [ContributesRegistration](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/pkg/pluginsdk/types.go#L105-L107) to ensure that non leader tasks are performed.

## Implementation Details

When creating a [manager](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#Manager), it can be [configured](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#Options) to enable leader election. This way any [runnable](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#Runnable) added to it can implement the [LeaderElectionRunnable](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#LeaderElectionRunnable) to specify if it should run only if the pod is a leader.
This way we can ensure that translation runs on all pods but the deployer and status writer only run on the leader.

### Configuration
A user can disable leader election by the `controller.disableLeaderElection` helm flag that translates into settings read by the kgateway pod.

```bash
helm upgrade -i --namespace kgateway-system --version v2.0.3 kgateway oci://cr.kgateway.dev/kgateway-dev/charts/kgateway --set controller.disableLeaderElection=true
```

### Plugin

No plugin will be created or modified.

### Controllers / Runnables

- The controllers / runnables will be modified to implement the `LeaderElectionRunnable` interface :
  ```
  func NeedLeaderElection() bool
  ```
- The controllers / runnables will include static type assertions :
  ```
  var _ manager.Runnable = &ProxySyncer{}
  var _ manager.LeaderElectionRunnable = &ProxySyncer{}
  ```
The following runnables will run on all pods :
- [AgentGwSyncer](https://github.com/kgateway-dev/kgateway/tree/main/internal/kgateway/agentgatewaysyncer) : This exposes the AgentGateway xDS snapshot and should run on all pods
- [ProxySyncer](https://github.com/kgateway-dev/kgateway/tree/main/internal/kgateway/proxy_syncer) : This translates the CRs into the xDS snapshot and should run on all pods. The [section](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/internal/kgateway/proxy_syncer/proxy_syncer.go#L390-L427) that writes the resource status will be split into a separate runnable `StatusSyncer` which will run only on the leader

The following runnables will only run on the leader :
- [gatewayClassProvisioner](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/internal/kgateway/controller/gatewayclass_provisioner.go#L26) : This creates the GatewayClass
- [controllerReconciler](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/internal/kgateway/controller/controller.go#L497) : This reconciles the GatewayClass
- [gatewayReconciler](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/internal/kgateway/controller/gw_controller.go#L23) : This reconciles the CRs by deploying the necessary resources (gateway deployment, service, etc.)
- [inferencePoolReconciler](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/internal/kgateway/controller/inferencepool_controller.go#L20) : This also reconciles the CRs by deploying the necessary resources
- StatusSyncer : This will be a new controller split from the ProxySyncer that will only handle writing status.

#### Status Syncer :
This new controller will only manage writing status. This will be split from the ProxySyncer.
The [krt policy collection register handlers](https://github.com/kgateway-dev/kgateway/blob/3567990b21d4ee9bf37985754891efc6a3917f18/internal/kgateway/proxy_syncer/proxy_syncer.go#L379-L385) will also be extracted into the StatusSyncer, with the initialSync flag set to true. This ensures that when a pod becomes a leader, it will pick up the latest report from the queue rather than process all items.

### Deployer

The `gatewayReconciler` reconciles the gateway resources and is managed by the gatewayReconciler. This will not need any changes

### Test Plan

The testing plan will include unit to verify the feature.
Kubernetes e2e tests can be added however it is to be determined the overlap this will have with chaos testing.

## Alternatives
- Re-implement the prior leadership election in https://github.com/kgateway-dev/kgateway/pull/6926.
  The prior implementation did not handle leader election in the manager itself for the following reasons :
  - It does not expose the `OnStartedLeading`, `OnStoppedLeading` && `OnNewLeader` callbacks.
  - Kube API server unavailability results in the control-plane crashing as the pod can not renew / acquire the lease. Crashing control plane pods can cause an outage during scaling the proxy deployment as the proxy pods that come up cannot fetch any configs and all routes result in 404s. Ref: github.com/kgateway-dev/kgateway/pull/9563, https://github.com/kgateway-dev/kgateway/issues/7346.

## Open Questions
- Are there any other tasks performed by the kgateway pod to be considered ?
- Should [ContributesRegistration](https://github.com/kgateway-dev/kgateway/blob/c51fc67919c8e15c7fd3941ccdbc8e8812c6d434/pkg/pluginsdk/types.go#L105-L107) be considered a leadership task ? If not how to split the status writing into a leadership task ?
